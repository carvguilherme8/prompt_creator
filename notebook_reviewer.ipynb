{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"scripts/\")\n",
    "\n",
    "from scripts_reviewer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Reviewer\n",
    "\n",
    "Nesse notebook vamos criar os prompts de inicialização do agente 'Reviewer'. Os cripts se encontram em 'scripts/script_reviewer'\n",
    "\n",
    "Abaixo teremos os prompts foram salvos para uso, caso deseje criar prompts novos, basta rodar as células novamente.\n",
    "\n",
    "Caso opte por criar novos prompts e o resultado não o agradar, convém executar a célula novamente (execuções anteriores não influenciam a atual).\n",
    "\n",
    "O prompt inicial é crucial. É recomendado que ele seja mais simples, assim as execuções de `create_following_promps` serão mais intuitivas.\n",
    "\n",
    "Por fim, caso deseje alterar a mensagem inicial de `create_first_prompt`, basta acessar o arquivo mencionado anteriomente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clarity for Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_clarity = create_first_prompt(\"clarity\")\n",
    "prompt_1_clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'clarity',\n",
       " 'valor': 2,\n",
       " 'prompt': 'Please review this CSV Cleaning Program Logic. I am unsure of its functionality and need feedback on how to improve the code. Specifically, tell me about any issues and suggested improvements when applying data cleaning best practices.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"I found this prompt not very clear to understand, plase make it a BIT more clearer. Don't include things like  [insert your Code Here]\"\n",
    "prompt_2_clarity = create_following_prompts(prompt_1_clarity, message)\n",
    "prompt_2_clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'clarity',\n",
       " 'valor': 3,\n",
       " 'prompt': \"Please review the attached CSV file cleaning program logic. I'm uncertain about its efficiency and structure, and need help identifying potential areas for improvement. Focus on addressing issues from a data cleaning best practices standpoint. Here are some specific questions: \\n\\n* **Data Integrity**: Are the cleaning steps effectively handling missing values or any inconsistencies in data input? Is there potential for biased sampling that might affect results \\n* **Efficiency**:  Do the operations lead to redundant outputs or an excessively lengthy processing time? Can the code be streamlined or optimized?\\n* **Clean Code Conventions**:Is the code well organized and written with clear variable naming convention, documented functions with informative comments etc? Are there potential for readability issues and maintainability problems. \\n\\nPlease provide your analysis and recommended suggestions to improve the program's functionality.\\n\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_3_clarity = create_following_prompts(prompt_2_clarity, message)\n",
    "prompt_3_clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_1_clarity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompts_clarity \u001b[38;5;241m=\u001b[39m [\u001b[43mprompt_1_clarity\u001b[49m, prompt_2_clarity, prompt_3_clarity]\n\u001b[0;32m      2\u001b[0m add_prompt_to_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompts_clarity)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt_1_clarity' is not defined"
     ]
    }
   ],
   "source": [
    "prompts_clarity = [prompt_1_clarity, prompt_2_clarity, prompt_3_clarity]\n",
    "add_prompt_to_json(\"clarity\", prompts_clarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity for Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'complexity',\n",
       " 'valor': 1,\n",
       " 'prompt': 'I need you to act as a code reviewer and analyze the provided Python code snippet to see if it cleans a CSV database. The code should be formatted by line ending with a space. Consider what might need revision or can improve efficiency in this script. Please provide your feedback in response with explanations for changes.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1_complexity = create_complexity_prompts(1)\n",
    "prompt_1_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'complexity',\n",
       " 'valor': 2,\n",
       " 'prompt': \"I need you to act as a code reviewer. Please analyze this Python code that performs basic data cleaning on a CSV database and tell me: \\n\\n* **Functionality**: Does the code successfully clean the data? Are there any potential issues or areas for improvement?\\n* **Code Quality**: How would you rate this code's style, readability, and efficiency? Suggest any improvements to make it more robust, readable, and maintainable.\\n* **Best Practices**: Are there any specific Python best practices or coding standards that should be applied to improve this code further?\\n\\n**Please consider the following when reviewing the code:** \\n1. It cleans a CSV string using pandas.\\n2. It uses numpy arrays for manipulation.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_2_complexity = create_complexity_prompts(2)\n",
    "prompt_2_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'complexity',\n",
       " 'valor': 3,\n",
       " 'prompt': \"I need you to act as a code reviewer.  Please look over this Python script and identify any potential issues that might arise during data cleaning in a CSV database with Pandas, Dask, or SQLAlchemy. I'm particularly interested in: \\n\\n* **Code Structure**: Is the code well-organized? Could any improvements be made to enhance readability?\\n* **Data Handling:** Are there any sections of the script that could benefit from being more structured for better data manipulation and flow within it?\\n* **Efficiency & Optimization:** What are some strategies to make this code more efficient, especially in handling potentially large CSV datasets?\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_3_complexity = create_complexity_prompts(3)\n",
    "prompt_3_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_complexity = [prompt_1_complexity, prompt_2_complexity, prompt_3_complexity]\n",
    "add_prompt_to_json(\"complexity\", prompts_complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size for Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'size',\n",
       " 'valor': 1,\n",
       " 'prompt': 'I need you to act as a code reviewer and analyze the following Python script, which attempts to clean a CSV database. Let me know your perspective on the logic and any potential improvements you notice. Be concise but thorough in your feedback.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1_size = create_first_prompt(\"size\")\n",
    "prompt_1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'size',\n",
       " 'valor': 2,\n",
       " 'prompt': \"Please review the provided Python script designed to clean a CSV database. Analyze the code's structure, logic for data processing, and identify potential areas for improvement in terms of efficiency, clarity or best practices. Please focus on brevity while providing comprehensive feedback, aiming to leave no stone unturned in your analysis. While brevity is appreciated, thoroughness ensures actionable insights can be extracted from all aspects of the code implementation.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"add about 50 words to this prompt to make it bigger\"\n",
    "prompt_2_size = create_following_prompts(prompt_1_size, message)\n",
    "prompt_2_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'size',\n",
       " 'valor': 3,\n",
       " 'prompt': \"Please review the provided Python script designed to clean a CSV database and analyze its effectiveness. Analyze the code's structure, logic for data processing, and **identify potential areas for improvement using best practices.** This includes considering efficiency, clarity, error handling, code readability, and scalability.**  **Focus on:** \\n- **Brevity** in your analysis, while providing **comprehensive insights**.  \\n- **Actionable steps** to improve the code's overall performance.\\n   **Note:** The script provides a framework. You should strive to generate clean, efficient code that is easily adaptable and scalable for potential future expansions or new data inputs. \"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_3_size = create_following_prompts(prompt_2_size, message)\n",
    "prompt_3_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_size = [prompt_1_size, prompt_2_size, prompt_3_size]\n",
    "add_prompt_to_json(\"size\", prompts_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specificity for Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'specificity',\n",
       " 'valor': 1,\n",
       " 'prompt': \"I need you to act as a code reviewer and look at this Python code. I'm hoping for your feedback on potential bugs or areas where the code could be improved in any way, shape, or form. Pay special attention to data handling and error-free execution!  Can you help review it? Please let me know!\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1_specificity = create_first_prompt(\"specificity\")\n",
    "prompt_1_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'specificity',\n",
       " 'valor': 2,\n",
       " 'prompt': \"Please review this Python code snippet and provide me with your feedback on potential bugs or areas for improvement.\\n\\nSpecifically, I'm interested in how the code handles data and ensures reliable error handling. Could you give specific suggestions as to what needs to be improved? How can I make it more robust? Thanks!\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"Can you make this prompt a little more specific??\"\n",
    "prompt_2_specificity = create_following_prompts(prompt_1_specificity, message)\n",
    "prompt_2_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'propriedade': 'specificity',\n",
       " 'valor': 3,\n",
       " 'prompt': 'Please review the following Python code snippet (below) and provide your feedback on potential bugs or areas for improvement: \\n\\n* **How does the code handle data input securely and ensure error handling is appropriate?** \\n* **Suggest specific ways to make it more robust:** \\n    * How can you enhance data validation? \\n    * Suggest any ways to optimize performance if required.\\n    * Are there potential security vulnerabilities I could be aware of?'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_3_specificity = create_following_prompts(prompt_2_specificity, message)\n",
    "prompt_3_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_specificity = [prompt_1_specificity, prompt_2_specificity, prompt_3_specificity]\n",
    "add_prompt_to_json(\"specificity\", prompts_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
